{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07829105",
   "metadata": {},
   "source": [
    "### Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c51363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Libraries\n",
    "import random\n",
    "import math\n",
    "import numbers\n",
    "import platform\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Importing essential libraries for basic image manipulations.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as tF\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0dda42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Enable/Disable GPU \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5869f1",
   "metadata": {},
   "source": [
    "### Functions for transforming CNN annotation_file data to inputable data for DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f35ed853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_dnn(champion_label_df, champion_label_df_count):\n",
    "\n",
    "    s_list = []\n",
    "    o_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(len(champion_label_df_count)):\n",
    "        youtuber = champion_label_df_count.iloc[i, :].youtuber\n",
    "        video_name = champion_label_df_count.iloc[i, :].video_name\n",
    "        frame_name = champion_label_df_count.iloc[i, :].frame_name\n",
    "        outcome = champion_label_df_count.iloc[i, :].outcome\n",
    "\n",
    "        subdf = champion_label_df[(champion_label_df.youtuber == youtuber)&(champion_label_df.video_name == video_name)&(champion_label_df.frame_name == frame_name)]\n",
    "        subdf = subdf.loc[:, ['cropped_name', 'predicted']]\n",
    "\n",
    "\n",
    "        input_s = np.zeros(shape=(28,))\n",
    "        input_o = np.zeros(shape=(28,))\n",
    "\n",
    "        y_list.append(outcome)\n",
    "\n",
    "        for j in range(len(subdf)):\n",
    "            cropped_name = subdf.iloc[j, :].cropped_name\n",
    "            predicted = subdf.iloc[j, :].predicted\n",
    "\n",
    "\n",
    "            # Find which player\n",
    "            player_type = re.search(r'S|O', cropped_name)\n",
    "            if player_type:\n",
    "                player_type = player_type[0]\n",
    "            else:\n",
    "                print('something wrong')\n",
    "            \n",
    "            champ_onehot = master_champ_list.index(predicted)\n",
    "\n",
    "\n",
    "            # Find which board index\n",
    "            board_index = re.search(r'\\d+', cropped_name)\n",
    "            if board_index:\n",
    "                board_index = int(board_index[0])\n",
    "            else:\n",
    "                print('something wrong')\n",
    "\n",
    "            if player_type == 'S':\n",
    "                input_s[board_index] = champ_onehot\n",
    "            elif player_type == 'O':\n",
    "                input_o[board_index] = champ_onehot\n",
    "\n",
    "        input_s = input_s.reshape((4, 7)).astype(int)\n",
    "        input_o = input_o.reshape((4, 7)).astype(int)\n",
    "        s_list.append(input_s)\n",
    "        o_list.append(input_o)\n",
    "\n",
    "    s, o, y = np.array(s_list), np.array(o_list), np.array(y_list)\n",
    "\n",
    "    return s, o, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47d8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_customimagedataset(s, o, y, test_size):\n",
    "    n = len(s)\n",
    "    # Calculate where to split\n",
    "    test_start_idx = int(np.ceil(test_size * n))\n",
    "    # All indices of data\n",
    "    indices = np.arange(0, n)\n",
    "    # Shuffle indices array\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "\n",
    "    train_indices = indices[test_start_idx:]\n",
    "    test_indices = indices[:test_start_idx]\n",
    "\n",
    "    s_train = s[train_indices]\n",
    "    s_test = s[test_indices]\n",
    "    o_train = o[train_indices]\n",
    "    o_test = o[test_indices]\n",
    "\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return s_train, s_test, o_train, o_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "323a7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, s, o, y):\n",
    "        \n",
    "        self.s = s\n",
    "        self.o = o\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.s)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        board_s = self.s[idx]\n",
    "        board_o = self.o[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        return board_s, board_o, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d7f11",
   "metadata": {},
   "source": [
    "### WIN PREDICTOR NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060fabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes in cmp which is 2 dimensional \n",
    "# [ [10, 8],\n",
    "#   [84, 3] ]\n",
    "# Where i, j is the one hot representation of the champions on ith row, jth col\n",
    "# trained_vec is a dictionary that convert each champions into a vector formate of size 2\n",
    "\n",
    "def pretrain_init(cmp, champ2vec):\n",
    "    \n",
    "    def vectoring_champ(x):\n",
    "        return champ2vec.get(x)\n",
    "    \n",
    "    return np.vectorize(vectoring_champ)(cmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f379d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Win_Predictor_Net(nn.Module):\n",
    "    def __init__(self, criterion, \n",
    "                 cmp_size = 85, embedding_size = 4, hidden_size_1 = 25, hidden_size_2 = 10, pre_train = False):\n",
    "        super(Win_Predictor_Net, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size_1 = hidden_size_1\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.cmp_size = cmp_size\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        # Embeddings to learn for champions \n",
    "        self.layer_cmp_emb = nn.Embedding(\n",
    "            num_embeddings=self.cmp_size+1,\n",
    "            embedding_dim=self.embedding_size,\n",
    "            padding_idx=85)#the onehot representation for background (check later)\n",
    "        \n",
    "        # Embeddings to learn for champions \n",
    "        self.layer_cmp_emb = nn.Embedding(\n",
    "            num_embeddings=self.cmp_size+1,\n",
    "            embedding_dim=self.embedding_size,\n",
    "            padding_idx=85)#the onehot representation for background (check later)\n",
    "\n",
    "\n",
    "\n",
    "        self.layer_w_0_s = nn.Linear(\n",
    "            in_features=self.embedding_size*28,\n",
    "            out_features=self.hidden_size_1,\n",
    "            bias=True)\n",
    "        \n",
    "        self.layer_w_0_o = nn.Linear(\n",
    "            in_features=self.embedding_size*28,\n",
    "            out_features=self.hidden_size_1,\n",
    "            bias=True)\n",
    "\n",
    "        self.layer_w_1 = nn.Linear(\n",
    "            in_features=2*self.hidden_size_1,\n",
    "            out_features=2*self.hidden_size_2,\n",
    "            bias=True)\n",
    "\n",
    "        self.layer_w_2 = nn.Linear(\n",
    "            in_features=2*self.hidden_size_2,\n",
    "            out_features=1,\n",
    "            bias=True)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "        # Pretrained embeddings\n",
    "        if pre_train:\n",
    "            emb_train = np.load(\"data/emb_train.npy\")\n",
    "            self.layer_cmp_emb.weight.data.copy_(torch.from_numpy(emb_train))\n",
    "            \n",
    "    def forward(self, scmp, ocmp):\n",
    "\n",
    "        # scmp --> torch.Size([1, 4, 7])\n",
    "        # ocmp --> torch.Size([1, 4, 7])\n",
    "\n",
    "        E_self = self.layer_cmp_emb(scmp)\n",
    "        E_opp = self.layer_cmp_emb(ocmp)\n",
    "        \n",
    "        # scmp --> torch.Size([1, 4, 7, self.embedding_size])\n",
    "        # ocmp --> torch.Size([1, 4, 7, self.embedding_size])\n",
    "\n",
    "        # SELF SIDE\n",
    "        s = E_self.view(-1, self.embedding_size)\n",
    "        # s --> torch.Size([28, self.embedding_size])\n",
    "        s = torch.flatten(s)\n",
    "        # s --> torch.Size([28 x self.embedding_size])\n",
    "        s = torch.nn.LeakyReLU()(self.layer_w_0_s(s))\n",
    "        # s --> torch.Size([self.hidden_size_1])\n",
    "\n",
    "        # OPPONENT SIDE\n",
    "        o = E_opp.view(-1, self.embedding_size)\n",
    "        # o --> torch.Size([28, self.embedding_size])\n",
    "        o = torch.flatten(o)\n",
    "        # o --> torch.Size([28 x self.embedding_size])\n",
    "        o = torch.nn.LeakyReLU()(self.layer_w_0_o(o))\n",
    "        # o --> torch.Size([self.hidden_size_1])\n",
    "        \n",
    "        # concat SELF AND OPPONENT\n",
    "        concat = torch.cat((s, o), axis = 0)\n",
    "        # concat --> torch.Size([2 x self.hidden_size_1])\n",
    "        \n",
    "        x = torch.nn.LeakyReLU()(self.layer_w_1(concat))\n",
    "        # x --> torch.Size([2 x self.hidden_size_2])\n",
    "\n",
    "        x = self.layer_w_2(x)\n",
    "        # x --> torch.Size([1])\n",
    "\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e402b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, num_epochs=25):\n",
    "    \n",
    "    since = time.time()\n",
    "    acc_list = []\n",
    "    model.train() # In training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs_s, inputs_o, labels in dataloaders:\n",
    "            inputs_s = inputs_s.to(device)\n",
    "            inputs_o = inputs_o.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs_s, inputs_o)\n",
    "            loss = model.criterion(outputs, labels)\n",
    "            \n",
    "            preds = torch.round(outputs)\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs_s.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Epoch information\n",
    "        epoch_loss = running_loss / len(dataloaders.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
    "        acc_list.append(epoch_acc)\n",
    "\n",
    "        print('Training Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    return acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d2b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloaders):\n",
    "    \n",
    "    since = time.time()\n",
    "    model.eval() # In training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs_s, inputs_o, labels in dataloaders:\n",
    "        inputs_s = inputs_s.to(device)\n",
    "        inputs_o = inputs_o.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs_s, inputs_o)\n",
    "            loss = model.criterion(outputs, labels)\n",
    "\n",
    "            preds = torch.round(outputs)\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs_s.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    overall_loss = running_loss / len(dataloaders.dataset)\n",
    "    overall_acc = running_corrects.double() / len(dataloaders.dataset)\n",
    "\n",
    "    print('Evaluation Loss: {:.4f} Acc: {:.4f}'.format(overall_loss, overall_acc))\n",
    "    \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    return overall_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817cea2",
   "metadata": {},
   "source": [
    "## Make data inputable:\n",
    "- input data should be (n, 4, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "041c763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with predicted labels:\n",
    "#   useful columns: \n",
    "#           youtuber --> (only Mortdog for now)\n",
    "#           video_name\n",
    "#           frame_name\n",
    "#           cropped_name --> for positional value\n",
    "#           predicted --> for converting to one-hot\n",
    "\n",
    "champion_label_df = pd.read_csv(os.path.join(os.getcwd(), 'data', 'final_data_set.csv'))\n",
    "\n",
    "with open(\"data/master_champ_list.pkl\", \"rb\") as input_file:\n",
    "        master_champ_list = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2562a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data_set.csv is the cleaned one\n",
    "df_to_fix = champion_label_df\n",
    "# Figure out the youtuber, video_name, frame_name keys that are valid for the input\n",
    "df_to_fix_count = df_to_fix.groupby(['youtuber', 'video_name', 'frame_name', 'outcome']).size().reset_index(name='img_count')\n",
    "df_to_fix_count = df_to_fix_count[df_to_fix_count.img_count == 56]\n",
    "\n",
    "df_to_fix_count = df_to_fix_count.loc[:, ['youtuber', 'video_name', 'frame_name', 'outcome']]\n",
    "\n",
    "s, o, y = format_for_dnn(df_to_fix, df_to_fix_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d569d7f",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4a5e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3429\n",
      "Test size: 858\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "epochs = 5\n",
    "\n",
    "s_train, s_test, o_train, o_test, y_train, y_test = before_customimagedataset(s, o, y, test_size = 0.2)\n",
    "\n",
    "print('Train size:', len(s_train))\n",
    "print('Test size:', len(s_test))\n",
    "\n",
    "trainset = CustomImageDataset(s_train, o_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "testset = CustomImageDataset(s_test, o_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ba3a7",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2237b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_predictor = Win_Predictor_Net(nn.BCELoss(), pre_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_SGD = torch.optim.SGD(win_predictor.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = train_model(win_predictor, trainloader, optimizer_SGD, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e3ef9",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(win_predictor, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee6a12",
   "metadata": {},
   "source": [
    "## NO EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b80af",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_predictor_noemb = Win_Predictor_Net(nn.BCELoss(), pre_train=False)\n",
    "optimizer_SGD_noemb = torch.optim.SGD(win_predictor_noemb.parameters(), lr=0.01, momentum=0.9)\n",
    "model_acc = train_model(win_predictor_noemb, trainloader, optimizer_SGD_noemb, epochs)\n",
    "eval_model(win_predictor_noemb, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9043af2",
   "metadata": {},
   "source": [
    "## Data for Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c295db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_train, s_test, o_train, o_test, y_train, y_test\n",
    "\n",
    "s_train_lr = s_train.reshape(len(s_train),4*7)\n",
    "s_test_lr = s_test.reshape(len(s_test),4*7)\n",
    "\n",
    "o_train_lr = o_train.reshape(len(o_train),4*7)\n",
    "o_test_lr = o_test.reshape(len(o_test),4*7)\n",
    "\n",
    "X_train_lr = np.concatenate([s_train_lr, o_train_lr], axis = 1)\n",
    "X_test_lr = np.concatenate([s_test_lr, o_test_lr], axis = 1)\n",
    "\n",
    "y_train_lr = y_train\n",
    "y_test_lr = y_test\n",
    "\n",
    "n_train = len(y_train_lr)\n",
    "n_test = len(y_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2356f7c9",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a3c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=0).fit(X_train_lr, y_train_lr)\n",
    "\n",
    "pred_logreg = logreg.predict(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39bbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_test_lr.reshape(n_test,1) == pred_logreg.reshape(n_test,1))/n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1228f1c",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00018125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# Initialize SVM classifier\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf = clf.fit(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svm = clf.predict(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_test_lr.reshape(n_test,1) == predictions_svm.reshape(n_test,1))/n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23dd60",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ca27063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2d1b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knc = KNeighborsClassifier(n_neighbors=2)\n",
    "knc.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "predictions_knc = knc.predict(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6a2d975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6282051282051282"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test_lr.reshape(n_test,1) == predictions_knc.reshape(n_test,1))/n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474e12d",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4994da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "dtc.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "predictions_dtc = dtc.predict(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_test_lr.reshape(n_test,1) == predictions_dtc.reshape(n_test,1))/n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98fbb20",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa464e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "rfc.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "predictions_rfc = rfc.predict(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bb0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_test_lr.reshape(n_test,1) == predictions_rfc.reshape(n_test,1))/n_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
